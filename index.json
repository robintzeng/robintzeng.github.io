[{"authors":["admin"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://robintzeng.github.io/author/jing-an-tzeng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jing-an-tzeng/","section":"authors","summary":"","tags":null,"title":"JING-AN TZENG","type":"authors"},{"authors":null,"categories":null,"content":"Abstract In this project, the main goal is to investigate the object detection model architecture to improve the mean Average Precision (mAP). In the past, researchers are dedicated to proposing new ideas in each part of the network architecture to enhance model performance. However, each method is a stand-alone little piece. It remains unclear if we can take advantage of these little pieces to push the model performance. Therefore, we aim to combine these refinements made in each architecture to investigate whether the model performance can be even better. Our modifications are based on Faster R-CNN with Feature Pyramid Network (FPN) to improve the model performance, including backbone, Region Proposal Network, feature maps refinement and RoI head.\nOur code repository is located here.\nPerformance Evaluate the performance on the PASCAL 2007 dataset     ","date":1609286400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609286400,"objectID":"da96979af073d1e0179f97ad60099b5d","permalink":"https://robintzeng.github.io/project/rcnn/","publishdate":"2020-12-30T00:00:00Z","relpermalink":"/project/rcnn/","section":"project","summary":"Modified the backbone, detection head and RPN to imporve the performace of Faster R-CNN","tags":["Computer Vision","Machine Learning"],"title":"Improvements on Object Detection (Faster R-CNN)","type":"project"},{"authors":null,"categories":null,"content":"Abstract When evaluating video inpainting methods, people often use different video sources. Commonly-used datasets are video segmentation datasets like DAVIS 2017 and YouTube-VOS because they provide foreground masks. However, they are not specifically designed for video inpainting: they don’t give labels to indicate whether the video is hard for video inpainting, and if so, why.\nFor this reason, we propose attributes that are likely to affect the video inpainting model performance — camera motion, foreground motion, background scene motion, and foreground displacement — and label them on DAVIS 2017. This can help us evaluate those videos’ difficulties for the video inpainting task. However, labeling a video objectively is a challenge for humans since everyone has their own criterion; therefore, we propose a set of automatic classifiers. We hope the new-annotated dataset will be a guide for researchers to improve their algorithms.\nClassifier Performance In the new dataset, we divide the videos’ motion into binary labels. It will be either high motion video or low motion video and evaluate the performance on the DAVIS dataset.\n Camera motion     In low camera motion video, most of the background textures are observed in both frames and that they are completely different on the right.   Camera Motion Performance    Foreground motion   In a low foreground motion video, the foreground object’s appearance is consistent across the frames. However, in high foreground motion video, the object tends to change its posture very much.   Foreground Motion Performance     Background scene motion   The high scenic motion video will obtain moving components like water in the background, and the low scenic motion video will obtain static backgrounds like rock and trees.   Background Scene Motion Performance     Foreground displacement   The high foreground displacement videos can often reveal different parts of the occluded background over time.\n    Foreground Displacement Performance   ","date":1609027200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609027200,"objectID":"f5bbfce29ec9d9c922574eddd810041c","permalink":"https://robintzeng.github.io/project/inpainting/","publishdate":"2020-12-27T00:00:00Z","relpermalink":"/project/inpainting/","section":"project","summary":"Divide the videos’ motion into binary labels and evaluated those videos’ difficulties for the video inpainting task.","tags":["Computer Vision"],"title":"Video Inpainting Benchmark","type":"project"},{"authors":null,"categories":null,"content":"   Fig. 1 Two Pathway   Abstract We develop the learning architecture that can effectively complete the dense depth from a color image and sparse LiDAR data.\nOur model consists of two pathways: the local pathway and the global pathway which is illustrated in Fig 1. The local pathway aims to extract high-resolution features, and it is made up of 2D blocks, which is illustrated in Fig. 2(b). The global pathway extracts low-resolution features, and it comprises our proposed U-Block, as shown in Fig. 2(a). The structure of the pathway is illustrated in Fig 3.\nAlso, we improve the performance of the local pathway by concatenating binary mask to the sparse LiDAR data, because the binary mask can help our model to indicate the valid values of sparse LiDAR data.\nFinally, to combine the results of the local and global pathways, we apply the attention mechanism (confidence map) to integrate the predicted dense from two pathways.\n        ","date":1585267200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585267200,"objectID":"32afb1efea5459606844e8bd6b4cd8c4","permalink":"https://robintzeng.github.io/project/cv/","publishdate":"2020-03-27T00:00:00Z","relpermalink":"/project/cv/","section":"project","summary":"Developed a learning structure to estimate a dense depth image from sparse depth measurements.","tags":["Depth Completion","Computer Vision","Machine Learning"],"title":"Depth Completion","type":"project"},{"authors":null,"categories":null,"content":"Abstract This project aims to adopt an open-source visual inertial navigation framework with two modifications. First, an alternative visual descriptor and extractor, SuperPoint, is tested in place of ORB features. Secondly, an invariant error state is implemented in order to more effectively track the IMU pose uncertainty while maintaining the visual feature correction step in a ”dual track” system. Each of these changes is evaluated separately on three datasets obtained from the EuRoC MAV dataset. Our code repository is located here.\nPerformance Evaluate the performance on the EuRoC MAV dataset           Video   ","date":1585267200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585267200,"objectID":"a4e8de353dbd62a4748cd44f8ec11310","permalink":"https://robintzeng.github.io/project/mobile/","publishdate":"2020-03-27T00:00:00Z","relpermalink":"/project/mobile/","section":"project","summary":"Navigated the drone with Openvins and the  learning based feature extractor -- Superpoint","tags":["Robotics","Computer Vision","Machine Learning"],"title":"Visual Inertia Navigation","type":"project"},{"authors":null,"categories":null,"content":"   Robots     Map   Abstract Created in 1998, Eurobot is an international robotics contest.\nFor each team, we have to make two robots that can cooperate to finish the tasks on the given map such as throwing the ball into a bucket and stacking the color bricks in the given order.\nWhen the robots finish the task, they would get some points. The team with higher scores wins.\nI was the software team leader in this contest. In our team, we took charge of sensing the world and making the decision for the robot. In addition, I made an indoor positioning system with UWB and Kalman filter which can locate the rival accurately.\nVideo   ","date":1527379200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527379200,"objectID":"3c40cab41ec234c2fc597aff83eaa71f","permalink":"https://robintzeng.github.io/project/eurobot/","publishdate":"2018-05-27T00:00:00Z","relpermalink":"/project/eurobot/","section":"project","summary":"Biult two robots and solved the tasks in Eurobot which is an international robotics contest in Europe annually.","tags":["Eurobot","Robotics"],"title":"Eurobot 2018","type":"project"},{"authors":null,"categories":null,"content":"Abstract To deal with the shoulder rehabilitation problem, we made an exoskeleton suit.\nIn this project, there are two parts of the exoskeleton. The sensing part and the activation part, the sensing part would fetch the information about the rehabilitation exercise from the therapist.\nAnd then, it will send that information to the activating part to practice the action that recorded.\nThus, the physical therapist can record his motion to the exoskeleton and let the patient do it many times.\nAlso, we made a GUI for the user to record and see whether their gesture is correct or not.       ","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"5afa1637b61b0928ce954bda23588606","permalink":"https://robintzeng.github.io/project/exo/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/exo/","section":"project","summary":"Seeked a new way to deal with the shoulder rehabilitation problem","tags":["Exoskeleton","Robotics"],"title":"Exoskeleton","type":"project"}]